\documentclass[a4,12pt]{article}

%--- Packages génériques ---%

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[babel=true]{csquotes}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{textcomp}
\usepackage{float}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{empheq}
\usepackage{color}
\usepackage{cancel}

%--- Structure de la page ---%

\usepackage{fancyheadings}

\topmargin -1.5 cm
\oddsidemargin -0.5 cm
\evensidemargin -0.5 cm
\textwidth 17 cm
\setlength{\headwidth}{\textwidth}
\textheight 24 cm
\pagestyle{fancy}
\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\sl MSIAM 3A}}
\chead[\fancyplain{}{{\sl }}]{\fancyplain{}{{}}}
\rhead[\fancyplain{}{}]{\fancyplain{}{Report}}
\lfoot{\fancyplain{}{}}
\cfoot{\fancyplain{}{}}
\cfoot{\thepage }
\rfoot{\fancyplain{}{}}

%--- Raccourcis commande ---%

\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\ub}{\mathbf{u}}

\DeclareMathOperator{\e}{e}
%--- Header to write code ---%
\usepackage { listings }
%%configuration de listings
\lstset{
	language=c++,
	basicstyle=\ttfamily\small, %
	identifierstyle=\color{black}, %
	keywordstyle=\color{blue}, %
	stringstyle=\color{black!60}, %
	commentstyle=\it\color{green!95!yellow!1}, %
	columns=flexible, %
	tabsize=2, %
	extendedchars=true, %
	showspaces=false, %
	showstringspaces=false, %
	numbers=left, %
	numberstyle=\tiny, %
	breaklines=true, %
	breakautoindent=true, %
	captionpos=b
}


\usepackage{xcolor}


\definecolor{Zgris}{rgb}{0.87,0.85,0.85}


\newsavebox{\BBbox}
\newenvironment{DDbox}[1]{
	\begin{lrbox}{\BBbox}\begin{minipage}{\linewidth}}
		{\end{minipage}\end{lrbox}\noindent\colorbox{Zgris}{\usebox{\BBbox}} \\
	[.5cm]}
%--- Mode correction et incréments automatiques ---%

\usepackage{framed}
\usepackage{ifthen}
\usepackage{comment}

\newcounter{Nbquestion}

\newcommand*\question{%
\stepcounter{Nbquestion}%
\textbf{Question \theNbquestion. }}

\newboolean{enseignant}
%\setboolean{enseignant}{true}
\setboolean{enseignant}{false}

\ifthenelse{
\boolean{enseignant}}{
\newenvironment{correction}{\begin{shaded}}{\end{shaded}}
}
{
\excludecomment{correction}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 							               EN-TETE        

\title{\textbf{Hybrid Data Assimilation}}
\author{
\begin{tabular}{cc}
	\textsc{Maxime Mazouth--Laurol} \& \textsc{Michi Rakotonarivo} \& \textsc{Ksenia Kozhanova}
\end{tabular}}   
\date{\small $16^{th}$ January 2018}

\makeatletter
	\def\thetitle{\@title}
	\def\theauthor{\@author}
	\def\thedate{\@date}
\makeatother 

\usepackage{etoolbox}
\usepackage{titling}
\setlength{\droptitle}{-7em}

\setlength{\parindent}{0cm}

\makeatletter
% patch pour le bug concernant les parenthèses fermantes d'après http://tex.stackexchange.com/q/69472
\patchcmd{\lsthk@SelectCharTable}{%
  \lst@ifbreaklines\lst@Def{`)}{\lst@breakProcessOther)}\fi}{}{}{}
\begin{document}

\maketitle

\section{Introduction}
When dealing with a numerical model, it should be kept in mind that it can never provide perfect estimation of complex physical models. While these models are constantly under development by many authors, to acquire as much as possible "realism" of real world dynamical system, they start to have a drawback of increasing uncertainty due to the growing sophistication of the underlying parameters. Even assuming the initial data without any sparse, the non-accurate representation of it can lead to the errors and, hence, the model will fail to provide an appropriate representation. One of the approaches to tackle this problem is using the parametrization. While there are variety of methods to perform parametrisation, we focus on variational data assimilation in this work.

This report presents several hybrid data assimilation methods, namely, ensemble 3DVAR (En3DVAR), ensemble 4DVAR (En4DVAR), the hybrid between extended Kalman filter and 3DVAR, and  Iterative Ensemble Kalman Smoother (IEnKS). The advantage to use hybrid methods is that we can benefit from the positive sides of several techniques. This can include, but not limit to, better estimation of the errors, lower computational cost, easier implementation etc. 

For each method described in this paper, we present some necessary for understanding the method information. Precisely, we provide the derivation of the 3D- and 4DVAR, present some background on Kalman theory and its extensions to the extended Kalman filter (EKF), and ensemble Kalman filter (EnKF).

\section{Ensemble Hybrid methods}

Since long, it had been understood that there is no possibility to derive the perfect forecast related, for example, to weather due to the fact that even small errors contained in the initial estimate grow large enough to make an attempt for the deterministic forecast useless. Epstein (1969), suggested a reasonable alternative by estimating the future states of probability density function (pdf)  of the atmosphere by the initial estimate.

Due to the complicated process of deriving the evolution of some complex model pdf, an ensemble forecasting (EF) has been suggested. The main idea behind EF is that each set of initial conditions and variety of numerical model options are used to generate several, individual numerical forecasts.

Data assimilation is the area in mathematics which is mainly arose due to the necessary developments in weather forecast processes. These processes require the numerical models initialization based on the relatives sparse observation information.  

Most data assimilation methods rely heavily on the background error covariance which is assumed to be homogeneous, isotropic, stationary and quasigeostropic in its structure (Daley 1991). It also uses some parameters which are taken from the innovation statistics (Hollingsworth and
Lonnberg 1986). While this makes the application of error covariance statistics relatively easy, it is no always appropriate, particularly under some unstable conditions.

One of the approaches is to use an ensemble forecast statistics in order to obtain the background error covariances dependent on the flow (Evensen 1994). These methods are normally based on the Kalman filter theory, introduced by Kalman in 1960, which predetermined the name of ensemble Kalmna filtering (EnKF).

Ensemble Kalman Filter (EnKF) has been first introduced by Evensen (1994b) and gained its wide use due to its relatively easy concepts, formulation and implementation. For instance, there is no requirements of derivation of neither tangent linear operator nor adjoint equations, and no integration backward in time.

On the other hand, variational techniques, like 3DVAR and 4DVAR, had been proven to be efficient schemes and, hence, hybrid methods using background error covariance statistics based on the ensemble and its application to variational approaches, can be the good choice.

These hybrid methods had been explored by quite few authors in the recent years. For example, Lorenc (2003), examined the possibility of making the background error covariance in the variation technique flow-dependent by preconditioning the control variable by perturbation of the background ensemble forecast.  Buehner (2005), implemented similar hybrid method into the global model data assimilation system using 3DVAR. Both of these authors suggested that good results can be obtained using the mix of 4DVAR and ensemble forecasts due to the fact, that 4DVAR gains optimal trajectory using non-sequential assimilation algorithm, which is essential in some applications, e.g. satellite observations (Simmons and
Hollingsworth 2002). 

This section presents the theory behind two hybrid schemes: ensemble 3DVAR and ensemble 4DVAR (En4DVAR). 

\subsection{3DVAR and 4DVAR assimilations}
This subsection is meant to give some background related to the 3D- and 4DVAR systems. In what is coming the notations based on Vialard et al. 2003 have been used.

We denote $\mathbf{w}$ as the state vector. 

Observations produce estimates of model unknowns which are then used to formulate the analysis $\mathbf{w^{a}}$. Previous analysis is used to generate background state $\mathbf{w^b}$. The background state is assumed to be relatively close to the state which we consider to be "true" and we can write the state vector $\mathbf {w}$ in terms background state and increment $\delta \mathbf {w}$:
\begin{align}
\mathbf{w} = \mathbf {w^{b}}+\delta \mathbf {w}
\end{align} 

Denoting $M$ as a model, $\mathbf {M} = \mathbf {M}(t_{i}, t_{i-1})$ as a linear operator related to $\delta \mathbf {w(t_{i-1})}$ and $M(t_{i},t_{i-1})$ as a non-linear model operator related to $\mathbf {w(t_{i-1})}$, we can write the relation between the vector $\mathbf {w}$ and model $M$ acting on $w$ over the time interval $[t_{i-1},t_{i}]$:

\begin{align}
\mathbf {w}(t_{i})=M(t_{i},t_{i-1})[\mathbf {w}(t_{i-1})]
\end{align}

We now substitute (1) into (2) and obtain 
\begin{align}
\mathbf {w}(t_{i}) \approx M(t_{i},t_{i-1})[\mathbf {w}^{b}(t_{i-1})] + \mathbf {M}(t_{i},t_{i-1})\delta \mathbf {w}(t_{i-1})
\end{align}

We then can define the increment prognostic model as following:

\begin{align}
\delta \mathbf {w}(t_{i}) = \mathbf {M}(t_{i},t_{i-1})\delta \mathbf {w}(t_{i-1})
\end{align}

Next, we denote $\mathbf {y}_{i}^{o}$ as the observation vector at $t_{i}$ and $H_{i}$ as the observation operator at $t_{i}$. We now can write the model relation of $H_{i}$ acting on $w$ at $t_{i}$:

\begin{align}
H_{i}[\mathbf {w}(t_{i})] \approx H_{i}[\mathbf {w}^{b}(t_{i})]+\mathbf {H}_{i}\delta \mathbf {w}(t_{i})
\end{align}

In what is to follow, we assume that observation vectors are available over $t_{0}\leq t_{i} \leq t_{n}$, $G_{i} = H_{i}M(t_{i},t_{0})$ being a generalized observation operator, $\mathbf {G}_{i} = \mathbf {H}_{i}\mathbf {M}(t_{i},t_{0})$ being a linear observation operator and hence, there is direct relation between the observation estimates and initial conditions:

\begin{align}
H_{i}[\mathbf {w}(t_{i})]=G_{i}[\mathbf {w}(t_{0})] \approx G_{i}[\mathbf {w}^{b}(t_{0})] + \mathbf {G}_{i}\delta \mathbf {w}(t_{0})
\end{align}

The 4DVAR system uses the analysis $\mathbf {w}^{a}=\mathbf {w}^{a}(t_{0})$ which simultaneously minimizes the distance to $\mathbf {w}^{b} = \mathbf {w}^{b}(t_{0})$ and to the observations $\mathbf {y}_{i}^{o}$ over the period $t_{0}\leq t_{i} \leq t_{n}$. We define the distance as an inner product $J^{F}$ where observations and accuracy taken into account, as the function of increment $\delta \mathbf {w} = \delta \mathbf {w}(t_{0})$:

\begin{align}
J^{F}(\delta \mathbf {w}) = \frac{1}{2} \delta \mathbf {w}^{T} \mathbf {B}^{-1} \delta \mathbf {w} + \frac{1}{2} \sum_{i=0}^{n}[G_{i}(\mathbf {w}^{b}+\delta \mathbf {w})-\mathbf {y}_{i}^{o}]^{T}R_{i}^{-1}[G_{i}(\mathbf {w}^{b} + \delta \mathbf {w}) - y_{i}^{o}]
\end{align}

where $\mathbf {B}$ and $\mathbf {R}_{i}$ are matrices with covariance estimates of background and observation error. We denote the background term which measures the fitting to $w^{b}$, $J_{b}$:

$$J_{b}=\frac{1}{2} \delta \mathbf {w}^{T} \mathbf {B}^{-1} \delta \mathbf {w}$$ 

and  the observation, $J_{o}^{F}$, which measures the fit between observation and model equivalent. 

$$J_{o}^{F}=\frac{1}{2} \sum_{i=0}^{n}[G_{i}(\mathbf {w}^{b}+\delta \mathbf {w})-\mathbf {y}_{i}^{o}]^{T}R_{i}^{-1}[G_{i}(\mathbf {w}^{b} + \delta \mathbf {w}) - y_{i}^{o}]$$

The analysis can be written as $\mathbf {w}{a} = \mathbf {w}^{b}+\delta \mathbf {w}^{a}$ and $\delta w^{a}$ is the minimization increment for $J^{F}$.

To make minimization procedure more simple and make it possible to find not many minima which come from the nonlinear component $G_{i}$ but unique one, Courtier et al. (1994) suggests to approximate $J^{F}$ by quadratic cost function $J$ using the linear operator $\mathbf {G}_{i}$:

\begin{align}
J(\delta \mathbf {w}) = \frac{1}{2} \delta \mathbf {w}^{T} \mathbf {B}^{-1} \delta \mathbf {w} + \frac{1}{2} \sum_{i=0}^{n}(\mathbf {G}_{i} \delta \mathbf {w} - \mathbf {d}_{i})^{T}\mathbf {R}_{i}^{-1}(\mathbf {G}_{i} \delta \mathbf {w} - \mathbf {d}_{i})
\end{align}
where $$J_{o} = \frac{1}{2} \sum_{i=0}^{n}(\mathbf {G}_{i} \delta \mathbf {w} - \mathbf {d}_{i})^{T}\mathbf {R}_{i}^{-1}(\mathbf {G}_{i} \delta \mathbf {w} - \mathbf {d}_{i})$$,

$$J_{b} = \frac{1}{2} \delta \mathbf {w}^{T} \mathbf {B}^{-1} \delta \mathbf {w}$$, and 

$\mathbf {d}_{i}$ stands for effective observation vector.

The main difference between 3DVAR and 4DVAR comes from the choice of $\mathbf{M}$ to obtain the linearized observation operator $\mathbf {G}_{i}$. While 3DVAR model simply uses $\mathbf {M} = \mathbf {I}$, where $\mathbf {I}$ corresponds to the identity matrix, i.e. we have persistence model, 4DVAR has $\mathbf {M} \approx (\partial M/\partial \mathbf {w})|_{w-w^{b}}$.

\subsection{Kalman filtering}
This section presents briefly the Kalman theory main principles.
The Kalman Filter is a method where the model requires only forward integration in time and it reinitialised every time the measurements are available. Here we use the following notations: $\psi^{f}$ is the model forecast, $\psi^{a}$ is the model analysis, $\textbf{d}$ is the vector containing the measurements, $\textbf{P}^{f}$ is the covariance for the model forecast, $\textbf{P}^{a}$ is the covariance for the model analysis and $\textbf{R}$ is the covariance for the model measurements, $\textbf{H}$ is the measurement operator relating to the true model state $\psi ^{t}$ to $\textbf{d}$ with possibility to have the measurement errors $\epsilon$. We now state the analysis equation:

$$\psi ^{a} = \psi ^{f} + \textbf{P} ^{f}\textbf{H} ^{T}(\textbf{HP} ^{f}\textbf{H} ^{T}+\textbf{R}) ^{-1}(\textbf{d-H}\psi) ^{f}$$

with

$$\textbf{P}^{a} = \textbf{P}^{f} - \textbf{P}^{f}\textbf{H}^{T}(\textbf{HP}^{f}\textbf{H}^{T}+\textbf{R})^{-1}\textbf{HP}^{f} $$

and 

$$\textbf{d} = \textbf{H}\psi{t} + \epsilon$$

We determine the reinitialisation $\psi^{a}$ as a weighted linear combination of $\psi^{f}$, $\textbf{P}^{f}$ and $\textbf{H}^{T}$. We can calculate the weights using the error covariance for the model prediction projected onto the measurements, the measurement error covariance and the innovation (i.e. the difference between prediction and measurements).     

We next define the Kalman gain matrix which is used to solve the equations related to the time evolution of the error covariance matrix of the model state

$\textbf{K} = \textbf{P}^{f}\textbf{H}^{T}(\textbf{H}\textbf{P}^{f}\textbf{H}^{T}+\textbf{R})^{-1}$

Further, writing the linear dynamical model in discrete form $\psi_{k+1}=\textbf{F}\psi{k}$, we have the following equation for the error covariance

$$\textbf{P}_{k+1}=\textbf{F}\textbf{P}_{k}\textbf{F}^{T}+\textbf{Q}$$
where \textbf{Q} is the model error covariance matrix. These errors normally arise due to the relaxation of physics and numerical approximations. Integrating the last two equations we get $\psi{f}$ and $\textbf{P}^{f}$. This process is known to be the Kalman Filter.

Contrary, writing the non-linear model for $\psi_{k+1}$

$$\psi_{k+1}=\textbf{f}(\psi_{k})$$

we obtain the Extended Kalman Filter with $\textbf{F}$ being the tangent linear operator, i.e. Jacobian, of $\textbf{f}(\psi)$.

\subsection{Ensemble Kalman filter}
We now switch to the ensemble Kalman filter presentation.

Using the true state to define the error covariance matrices for $\textbf{P}^{f}$ and $\textbf{P}^{a}$ using the expectation value 

$$\textbf{P}^{f} = \overline{(\psi^{f}-\psi^{t})(\psi^{f}-\psi^{t})^{T}}$$

$$\textbf{P}^{a} = \overline{(\psi^{a}-\psi^{t})(\psi^{a}-\psi^{t})^{T}}$$

where $\psi ^{f}$, $\psi ^{a}$, $\psi ^{t}$ are the model state vectors at certain time point for the forecast, analysis and true state, respectively.

The true state, $\psi ^{t}$, is not known, hence, we redefine above two equations using the ensemble covariance matrices around the mean value $\psi$ and average over ensemble. 

$$\textbf{P}^{f} \simeq \textbf{P}^{f}_{e} = \overline{(\psi^{f}-\bar{\psi^{f}})(\psi^{f}-\bar{\psi^{f}})^{T}}$$

$$\textbf{P}^{a} \simeq \textbf{P}^{a}_{e}= \overline{ (\psi^{a}-\bar {\psi^{a}})(\psi^{a}-\bar{\psi^{a}})^{T} }$$

Thus, we assume that the ensemble mean $\overline {\psi}$ is the best estimate  and we are looking for the error in ensemble mean as the spread around the mean.

Now, to define the ensemble Kalman filter analysis scheme using the definition of $\textbf{P}^{f}_{e}$ and $\textbf{P}^{a}_{e}$.

The ensemble of observations state:

$$\textbf{d}_{j} = \textbf{d} + \epsilon_{j}$$

and ensemble covariance matrix $\textbf{R}_{e}$ converges to $\textbf{R}$ as ensemble goes to infinity and defined as 

$$\textbf{R}_{e} = \overline{\epsilon \epsilon ^{T}}$$

We rewrite the analysis step which now is updated by every model state ensemble number

$$\psi ^{a} _{j}= \psi ^{f}_{j} + \textbf{P} _{e} ^{f}\textbf{H} ^{T}(\textbf{HP}_{e} ^{f}\textbf{H} ^{T}+\textbf{R}_{e}) ^{-1}(\textbf{d}_{j}-\textbf{H}\psi ^{f}_{j})$$

It is worth to note that the matrices $\textbf{H}\textbf{P}^{f}_{e}\textbf{H}^{T}$ and $\textbf{R}_{e}$ are singular as long as we have number of measurements larger than number of ensembles and hence, we have to use pseudo inversion. Also, the equation above for the $\psi ^{a} _{j}$ is approximation when we have finite number of ensemble members. Moreover, noting $\textbf{d} = \overline{\textbf{d}}$ as the initial guess vector of measurements, we have

$$\overline{\psi ^{a}} = \overline{\psi ^{f}} + \textbf{P} ^{f}_{e}\textbf{H} ^{T}(\textbf{HP}_{e} ^{f}\textbf{H} ^{T}+\textbf{R}_{e}) ^{-1}(\overline{\textbf{d}}-\textbf{H}\overline{\psi ^{f}})$$

Thus, we can see that we have similar relation between analysis and forecast in ensemble scheme as we do in Kalman filter, keeping in mind that we use ensemble mean in ensemble Kalman filter scheme.

Using the definition of Kalman gain $$\textbf{K}_{e} = \textbf{P}_{e}^{f}\textbf{H}^{T}(\textbf{H}\textbf{P}^{f}_{e}\textbf{H}^{T}+\textbf{R}_{e})^{-1}$$ and subtracting $\overline{\psi_{j}^{a}}$ from $\psi_{j}^{a}$ we have

$$\psi_{j}^{a}-\overline{\psi}^{a} = (\textbf{I}-\textbf{K}_{e}\textbf{H})(\psi_{j}^{f} - \overline{\psi}^{f})+\textbf{K}_{e}(\textbf{d}_{j}-\overline{\textbf{d}})$$

and we derive the error covariance as follows,

$$\textbf{P}_{e}^{a} = \overline {(\psi^{a} - \overline{\psi{a}})(\psi{a} - \overline{\psi^{a}})^{T}} = (\textbf{I}-\textbf{K}_{e}\textbf{H})\textbf{P}_{e}^{f}(\textbf{I}-\textbf{H}^{T}\textbf{K}_{e}^{T})+\textbf{K}_{e}\textbf{R}_{e}\textbf{K}_{e}^{T}=$$$$\textbf{P}_{e}^{f} - \textbf{K}_{e}\textbf{H}\textbf{P}_{e}^{f} - \textbf{P}_{e}^{f}\textbf{H}^{T}\textbf{K}_{e}^{T} + \textbf{K}_{e}(\textbf{H}\textbf{P}_{e}^{f}\textbf{H}^{T}+\textbf{R}_{e})\textbf{K}_{e}^{T} =$$$$ (\textbf{I}-\textbf{K}_{e}\textbf{H})\textbf{P}_{e}^{f}$$ 

The last expression is the Kalman filter minimum variance error covariance. Thus, if we have infinite number of ensembles, the ensemble Kalman filter provides the same result as Kalman filter and extended Kalman filter.

\subsection{En3DVAR}
This subsection presents the hybrid method based on the 3D variational schemes combined with EnKF.
Here we define $\textbf{x}$ to be the model state vector, $\textbf{y}^{o}$ the set of observations, $\textbf{x}^{b}$ the background forecast, $\textbf{x}^{a}$ the analysis. We formulate the cost function for the minimiser $x$ using 3DVAR as follows,

$$J(x) = \frac{1}{2}[(x-x^{b})^{T}\textbf{B}^{-1}(\textbf{x}-\textbf{x}^{b})+(\textbf{y}^{o}-\textbf{H}\textbf{x})^{T}\textbf{R}^{-1}(\textbf{y}^{o}-\textbf{Hx})]$$

where $\textbf{B}$ is the background error covariances estimation, $\textbf{R}$ is the matrix containing observation errors and $\textbf{H}$ is the mapping operator for the model state onto the observations and  $\textbf{R}$ is the measurement error covariance. The relation between $\textbf{x}^{t}$ and $\textbf{y}^{o}$ is, thus, $$\textbf{y}^{o} = \textbf{H}\textbf{x}^{t} + \epsilon$$ where $\epsilon$ is the random vector, distributed according to the normal distribution, with zero mean and covariance matrix $\textbf{R}$. The increment $\textbf{x}^{a} - \textbf{x}^{b}$ satisfies, 
$$(\textbf{I} + \textbf{BH}^{T}\textbf{R}^{-1}\textbf{H})(\textbf{x}^{a}-\textbf{x}^{b}) = \textbf{B}\textbf{H}^{T}\textbf{R}^{-1}(\textbf{y}^{o}-\textbf{H}\textbf{x}^{b})$$   

Under 3DVAR we use some assumptions to calculate $\textbf{B}$. Namely, (i) $\textbf{B}$ is fixed in time, (ii) $\textbf{B}$ is diagonal in horizontal spectral coefficients, i.e. $\textbf{B} = \textbf{S}\textbf{C}\textbf{S}^{T}$, where $\textbf{S}$ is the transformation for spectral coefficients to grid points and $\textbf{C}$ is the diagonal matrix consisting the spectral coefficients variances (iii) $\textbf{B}$ has horizontal and vertical structures which can be separated (Hamill and Snyder, 2000).

In En3DVAR scheme, we use the following formulation to compute the sample covariance matrix $\textbf{P}^{b}$,

$$\textbf{B} = (1 - \alpha)\textbf{P}^{b} + \alpha \textbf{S}\textbf{C}\textbf{S}^{T}$$

To compute $\textbf{P}^{b}$ in this way, we have an option to change the analysis from only flow-dependent, ensemble-based error covariances to original 3DVAR covariances by changing $\alpha$ from 0.0 to 1.0.

Implementation of the hybrid scheme requires to perform adequate ensemble. The procedure is, firstly, to use each ensemble member for the background forecast in analysis increment formulation and update it using some perturbed observations. To use this scheme, there are several approaches to calculate $\textbf{P}^{b}$. First one, and the easiest, is to treat $\textbf{P}^{b}$ as the sample covariance matrix based on the whole ensemble and update each member by the same background covariance. However, this way does not have sufficient performance in estimation of the background errors variance, which also leads to the weak performance of the EnKF. This can be avoided by using double EnKF, where we split the ensemble by two and use the sample covariance from one set to update the second one. Second approach, suggested by Hamill and Snyder (2000), is to calculate $\textbf{P}^{b}$ for the $i$th member based on the sample which does not have this member, 

$$\textbf{P}^{b}_{i} = (n-2)^{-1} \sum_{j=1, j \ne i}^{n}(\textbf{x}_{j}^{b} - \overline{\textbf{x}}_{i}^{b})(\textbf{x}_{j}^{b} - \overline{\textbf{x}}_{i}^{b})^{T}$$

where $n$ is the number of ensembles and  $\textbf{x}_{i}^{b}$ is the ensemble mean.

This hybrid technique gives an advantage to provide some reduction in analysis errors comparing with 3DVAR standing alone.

\subsection{En4DVAR algorithm}

En4DVAR scheme is essentially a combination between EnKF and 4DVAR schemes. Unlike traditional Kalman filter, EnKF is using Monte Carlo approach to approximate the covariance of the forecast error. Noting the number of ensemble members by $N$ and the state vector by $\textbf{x}$, we estimate the background error as following,

$$\textbf{X}'_{b}=\frac{1}{\sqrt{N-1}}(\textbf{x}_{b1}-\overline{\textbf{x}_{b}},\textbf{x}_{b2}-\overline{\textbf{x}_{b}},......,\textbf{x}_{bN}-\overline{\textbf{x}_{b}})$$

Next, we precondition the control variable of the variation,

$$\textbf{x}_{a} = \textbf{x}_{b} + \textbf{X}'_{b}\textbf{w}$$

with innovation,

$$\textbf{d} = H(\textbf{x}_{b})-\textbf{y}$$

Using control variable state $\textbf{w}$, we can write the cost function

$$J(\textbf{w}) = \frac{1}{2} \textbf{w}^{T}\textbf{w} + \frac{1}{2}(\textbf{H}\textbf{X}'_{b}\textbf{w}+\textbf{d})^{T}\textbf{O}^{-1}(\textbf{H}\textbf{X}'_{b}\textbf{w}+\textbf{d})$$

where innovation $\textbf{d}$ is . Rewriting the cost function using the background perturbation to precondition the 4DVAR control variables, we have the formulation En4DVAR cost function

$$J(\textbf{w}) = \frac{1}{2} \textbf{w}^{T}\textbf{w} + \frac{1}{2}\sum_{i=0}^{I}(\textbf{HM}\textbf{X}'_{b}\textbf{w} + \textbf{d}_{i})^{T}\textbf{O}^{-1}(\textbf{HMX}'_{b}\textbf{w}+\textbf{d}_{i})$$

Computing the gradient with respect to the control variables, we obtain

$$\nabla_{w}J = \textbf{w} + \sum_{i=0}^{I} \textbf{X}_{b}^{'T}\textbf{M}^{T}\textbf{H}^{T}\textbf{O}^{-1}(\textbf{HMX}'_{b}\textbf{w}+\textbf{d}_{i})$$

Similarly to EnKF, we use perturbation in observation space which is computed as follows in the case of EnKF,

$$\textbf{HX}'_{b} \approx \frac{1}{\sqrt{N-1}}(H\textbf{x}_{b1}-\overline{H\textbf{x}_{b}},H\textbf{x}_{b2}-\overline{H\textbf{x}_{b}},.....,H\textbf{x}_{bN}-\overline{H\textbf{x}_{b}})$$

we compute observation space background error

$$\textbf{HMX}'_{b} \approx \frac{1}{\sqrt{N-1}}(HM\textbf{x}_{b1}-HM\overline{\textbf{x}_{b}}, HM\textbf{x}_{b2}-HM\overline{\textbf{x}_{b}},.....,HM\textbf{x}_{bN}-HM\overline{\textbf{x}_{b}} )$$

hence, we have the recomputed gradient of the cost function

$$\nabla _{w}J = \textbf{w} + \sum_{i=0}^{I}(\textbf{HMX}_{b}')^{T}\textbf{O}^{-1}(\textbf{HMX}_{b}'\textbf{w}+\textbf{d}_{i})$$


One of the main advantages of En4DVAR is the relaxation of the requirement to have tangent linear model or adjoint model during the implementation. The experiments performed by Hamill and Snyder (2000) on the shallow water demonstrated that it can  provide the results similar to those which use both models with some benefits of the smaller computational cost. Moreover, En4DVAR proved to give reasonable results in comparison with 3DVAR, 4DVAR, EnKF schemes. Also, the non-sequential nature of En4DVAR also leads to the adequate data assimilation method accumulating some unbalance conditions (Liu et al, 2007).  


\subsection{IEnKS}
~~ Another data assimilation worth-considering in the context of Hybrid methods is the Iterative Ensemble Kalman Smoother(IEnKS), applied in \cite{jointState} to joint state and parameter estimation. In fact, it belongs to the group of Ensemble Variational methods(EnVar) and so is not considered as hybrid, but its purpose is to take advantages of both filtering and variational methods :
\begin{itemize}
\item As an adjoint-free model, the burden of computing huge matrices derivative is avoided
\item flow-dependent scheme
\end{itemize}
Plus, in the IEnKS, the states ensemble propagates through model state and observation operator using Gaussian assumptions. \\
Joint state and parameter estimation is the augmentation of the standard state vector with parameter variables of the studied system. As such parameters can be non-observable, data assimilation aims at finding covariances between them and observed data.
Here are briefly stated a few notations to understand how IEnKS works. \\
$\textbf{E}_0 = [x_{0,[1]},...,x_{0,[N]}]$ the ensemble of N state vectors at $t_0$. \\
$\bar{x_0} = mean\{x_{0,[i]}\}_{i \in [\![1,N]\!] }$ \\
$\textbf{A}_0 = [x_{0,[1]} - \bar{x_0},...,x_{0,[N]}- \bar{x_0}]$. is the first empirical moment vector.\\
Then, the goal is to find \textbf{w} such that $x_0 = \bar{x_0} + \textbf{A}_0 \textbf{w}$ is the most accurate initial state vector. The problem has arbitraly been reduced to the vector space described by $\bar{x_0}$ and $\textbf{A}_0$ \\
Hence, the below cost function to be minimized is defined : 
$$\tilde{J}(w) = \frac{1}{2}(N-1)\textbf{w}^{T}\textbf{w} + \frac{1}{2}\sum_{k=1}^{L}\beta_{k}\delta_{k}^{T}(\textbf{w})\textbf{R}_{k}^{-1}\delta_{k}(\textbf{w})$$

$$\delta_{k}(\textbf{w}) = \textbf{y}_{k} - \mathcal{H}_{k}\circ \mathcal{M}_{k\leftarrow 0}(\textbf{x}_{0}^{(0)} + \textbf{A}_{0}\textbf{w})$$

where $\mathcal{M}_{k\leftarrow 0}$ stands for the non-linear model operator from $t_{0}$ to $t_{k}$, $\mathcal{H}_{k}$ the observation operator, $\{\beta_k\}_{k \in [\![1,L]\!]}$ are observation weights belonging to $[0,1]$. 

We define the observation vector as $\textbf{y} \in \mathbb{R}^d$ and assume that it is obtained at every time step $\Delta t$. \\

$$\textbf{w}^{j+1} = \textbf{w}^{j} - \tilde{H}_{j}^{-1} \delta \tilde{J}_{j}(\textbf{w}^{(j)})$$

we can minimise the cost function iteratively starting from computing the gradient and approximating the full Hessian $\tilde{H}_{j}$,

$$\nabla \tilde{J}_{(j)} = -\sum_{k=1}^{L}\beta_{k}\textbf{Y}_{k,(j)}^{T}\textbf{R}_{k}^{-1}[\textbf{y}_{k} - H_{k}\circ M_{k\Leftarrow 0}(\textbf{x}_{0}^{(j)})] + (N-1)\textbf{w}^{(j)}$$

$$\tilde{H}_{j}=(N-1)\textbf{I}_{N}+\sum_{k=1}^{L} \beta_{k} \textbf{Y}_{k,(j)}^{T}\textbf{R}^{-1}_{k} \textbf{Y}_{k,(j)} $$

$$\textbf{x}_{0}^{(j)} = \textbf{x}_{0}^{(0)} + \textbf{A}_{0}\textbf{w}^{(j)}$$

The analysis in ensemble space is variational, but the gradient of the cost function is computed using the state ensemble.
\subsubsection{Algorithm outline for one cycle}
The idea behind the algorithm is to work with $L$ consecutive state vectors and observations, describing a Data Assimilation Window(DAW) of size $L$. Then, once the minimized solution for the background vector has been found in this DAW, the solution becomes the background itself for a $S$ steps forecast. \\
For a standard EnKF, these values are fixed to $L = S = 1$. \\

\paragraph{Initialization}
\begin{itemize}
\item set of coefficients $\textbf{w}_0$
\item define $x_0, E_0$ the background
\item define integers S and L, $S \le L$
\end{itemize}
\paragraph{Analysis step}
Until the minimization is satisfying enough
\begin{itemize}
\item $x_0 = \bar{x_0} + \textbf{A}_0 \textbf{w}$
\item propagate the state variable through $\mathcal{M}$ and $\mathcal{H}$ $L$ times.
\item Assimilate the $L$ corresponding observations $\{y_{k}\}_{k \in [\![1,L]\!] }$ while computing the approximated gradient and hessian of the minimization process
\item compute new minimization solution \textbf{w}
\end{itemize}
\paragraph{Forecast step}
Once the best solution for \textbf{w} has been found, one first update the initial ensemble $\textbf{E}^{*}_0 = \bar{x_0} + A_{0}w^{*}$. Then, an $S\Delta t$ forecast is achieved : $\textbf{E}^{*}_S = \mathcal{M}_{S\leftarrow 0}(\textbf{E}^{*}_0)$, and will then become the background for next analysis.


\subsubsection{Data assimilation monitoring}
From above formulae and statements, one can notice that the parameters to monitor the data assimilation process are $\{\beta_k\}_{k \in [\![1,L]\!]}$, $S$ and $L$. \\
$\forall k \in [\![1,L]\!]$, the value of $\beta_k$ stands for the importance of the observation $y_k$ in the assimilation. As an extreme value, $\beta_k = 0$ means that the data is not considered at all. \\
Then, the choices of $S$ and $L$, for $1 \le S \le L$, define whether the assimilation is single or multiple. For $S = L$,  the DAWs do not overlap, so each observation is used once and only once. For $S < L$, there is  $L-S$ observations that intersect two consecutive DAWs, and thus are assimilated at least two times. It also means that the number of iterations increases, so a balance needs to be found, namely depending on the size of the DAWs.

\subsubsection{Numerical experiments and results}
~~ In order to emphasize the efficiency of the IEnKS, numerical experiments are achieved, conducting twin experiments together with the following Lorenz-95 model : \\
\begin{equation*}
\frac{dx_m}{dt} = (x_{m+1}-x_{m-2})x_{m-1} - x_m + F
\end{equation*}
with $x_m, m \in [\![1,M]\!]$ a variable and F the forcing parameter. \\
\paragraph{problem setup}
The truth state through time is noised according to the observation error covariance matrix $\textbf{R}_k$, with a fully observed system ($\mathcal{H}_k = \textbf{I}_d$) 
Solving the dynamical system, the choice of $\Delta t$ leads to more or less strong nonlinearities. \\
Finally, the metric to demonstrate the method accuracy is the root mean square error (RMSE) :
$$\operatorname{RMSE}= \sqrt{\frac{\sum_{t=1}^M (x_{m}^{a} - x_{m}^{t})^2}{M}}.$$
Using such a metric as a function of the DAW length $L$, the IEnKS -for Single DA with S=1 and S=L and Multiple DA with S=1- is meant to be compared with EnKS and 4D-Var.

\paragraph{weak nonlinearity $\Delta t = 0.05$}
In this case, for small values of $L$ ($\leq 20$), the IEnKS methods are similarly accurate, independently of $S$. Plus, they are far more efficient than EnKS and 4D-Var. \\
However, for higher DAWs amplitudes, it turns out that 4D-Var becomes more and more accurate, so does MDA, whereas SDA errors strongly increase.

\paragraph{strong nonlinearity $\Delta t = 0.20$}
The results lead to the same conclusion as above, but the difference between MDA and 4D-Var is much more obvious.


\newpage
\bibliographystyle{plain}
\bibliography{report}
\end{document}